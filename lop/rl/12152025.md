# Progress Notes: Experiment Analysis and Visualization
**Date: December 15, 2025**

---

## 1. Completed Experiments

All 8 experiments finished successfully:

| Environment | Method | Job ID | Final Return | Dead Neurons |
|-------------|--------|--------|--------------|--------------|
| **Ant-v3** | CBP | 5228714 | ~4,401 | ~1.4% |
| **Ant-v3** | L2 | 5228718 | ~4,462 | ~3-4% |
| **Ant-v3** | NS Adam | 5228719 | ~3,006 | ~58% |
| **Ant-v3** | Standard PPO | 5228737 | **-383,981** (collapsed) | ~25% |
| **SlipperyAnt-v3** | CBP | 5228724 | ~4,071 | ~2% |
| **SlipperyAnt-v3** | L2 | 5228722 | ~3,098 | ~4% |
| **SlipperyAnt-v3** | NS Adam | 5228721 | ~1,586 | ~42% |
| **SlipperyAnt-v3** | Standard PPO | 5228738 | ~995 | ~25% |

**Key Findings:**
- **CBP** consistently performs best - maintains low dead neurons and highest returns
- **Standard PPO collapsed catastrophically** on Ant-v3 (negative millions returns)
- **NS Adam** accumulates the most dead neurons (58% on Ant, 42% on SlipperyAnt)
- **L2** is a reasonable middle ground

---

## 2. Visualization Script Created

Created `plot_all_metrics_v2.py` to generate all plots from log files and pickle data.

### Metrics Plotted

| Metric | Source | Plots Generated |
|--------|--------|-----------------|
| **Returns** | Output logs | `returns_ant.png`, `returns_sant.png` |
| **Dormant Units** | Output logs | `dormant_ant.png`, `dormant_sant.png` |
| **Stable Rank** | Pickle logs | `stable_rank_ant.png`, `stable_rank_sant.png` |
| **Weight Magnitude** | Pickle logs | `weight_magnitude_ant.png`, `weight_magnitude_sant.png` |
| **Predictive Margin Dead%** | Pickle logs | `margin_predictive_ant.png`, `margin_predictive_sant.png` |
| **Actual Margin Dead%** | Pickle logs | `margin_actual_ant.png`, `margin_actual_sant.png` |
| **Dormant vs Margin Comparison** | Both | `dormant_vs_margin_*.png` |

### Plot Style
- Colors: Standard PPO (red), CBP (blue), NS Adam (orange), L2 (purple)
- Y-axis starts from 0
- SlipperyAnt plots include friction change markers (dashed vertical lines every 2M steps)

---

## 3. Margin Analysis Bug Fix

### The Problem
Initially plotted `mean_margins` which showed negative values (not a percentage).

### The Fix
Changed to extract `dead_counts` and compute proper percentage:
```python
dead_pct = (total_dead / 512) * 100  # 512 = 256 + 256 neurons in 2 hidden layers
```

This now correctly shows dead neuron percentage (0-100%) matching WandB's `predictive_margin/dead_pct`.

---

## 4. Margin Calculation Verification

Verified the implementation in `run_ppo_wandb.py` is **CORRECT**:

### Timeline for PPO Update k:
```
Step 0 to bs-1: Collect observations into both buffers
    - PPO buffer: stores for training
    - window_obs_buffer: stores for margin analysis (M_k)
    - saved_policy_state = �_{k-1} (from previous update)

Step bs-1: PPO update triggers (inside agent.log_update)
    � weights become �_k

After agent.log_update returns:
    1. PREDICTIVE MARGIN: Load �_{k-1}, compute on M_k
    2. ACTUAL MARGIN: Use �_k, compute on M_k
    3. Save �_k for next iteration
```

### Predictive vs Actual Margin

| Metric | Weights | Data | Question Answered |
|--------|---------|------|-------------------|
| **Actual Margin** | �_k (AFTER training) | M_k | "Which neurons are dead NOW?" |
| **Predictive Margin** | �_{k-1} (BEFORE training) | M_k | "Which neurons WILL die on this data?" |

**Use case:** Predictive margin provides early warning - if neurons are at risk BEFORE training, you could intervene (e.g., reinitialize) to prevent death.

---

## 5. Dormant Units vs Margin Dead% Comparison

Created comparison plots showing both metrics on same axes.

### Key Differences

| Aspect | Dormant Units | Margin Dead% |
|--------|---------------|--------------|
| **Interval** | Every 1024 steps | Every 2048 steps (PPO update) |
| **Method** | Activity-based: fires d1% of time | Geometry-based: max(w@x + b) < 0 |
| **Window** | Discrete 1000-step windows | Rolling buffer of current window |
| **Noise** | Smoother (averaged) | Noisier (per-update) |

### Finding
Both metrics track **very closely**, validating they measure the same phenomenon (neurons dying) with different approaches.

---

## 6. Data Locations

### Output Logs
```
logs/5228714_ant_cbp.out    # Ant CBP
logs/5228718_ant_l2.out     # Ant L2
logs/5228719_ant_ns.out     # Ant NS Adam
logs/5228737_ant_std.out    # Ant Standard PPO
logs/5228721_sant_ns.out    # SlipperyAnt NS Adam
logs/5228722_sant_l2.out    # SlipperyAnt L2
logs/5228724_sant_cbp.out   # SlipperyAnt CBP
logs/5228738_sant_std.out   # SlipperyAnt Standard PPO
```

### Pickle Logs (contain margin_data, stable_rank, weights)
```
data/ant/cbp/0.log
data/ant/l2/0.log
data/ant/bp/std/0.log
data/ant/bp/ns/0.log
data/sant/cbp/0.log
data/sant/l2/0.log
data/sant/bp/std/0.log
data/sant/bp/ns/0.log
```

---

## 7. Generated Plots Summary

All plots saved to `/scratch/gautschi/shin283/loss-of-plasticity/lop/rl/`:

```
# Returns
returns_ant.png
returns_sant.png

# Dormant Units
dormant_ant.png
dormant_sant.png

# Stable Rank
stable_rank_ant.png
stable_rank_sant.png

# Weight Magnitude
weight_magnitude_ant.png
weight_magnitude_sant.png

# Margin Dead%
margin_predictive_ant.png
margin_predictive_sant.png
margin_actual_ant.png
margin_actual_sant.png

# Comparisons
dormant_vs_margin_actual_ant.png
dormant_vs_margin_actual_sant.png
dormant_vs_margin_predictive_ant.png
dormant_vs_margin_predictive_sant.png
```

---

## 8. Next Steps

1. **Analyze predictive vs actual margin correlation** - Does low predictive margin forecast actual death?
2. **Per-layer analysis** - Which layers lose neurons first?
3. **Run more seeds** - Current experiments only have seed 0
4. **Consider SlipperyAnt friction change timing** - Verify changes happen every 2M steps as expected

---

## 9. Analysis and Discussion (LaTeX Format for Overleaf)

```latex
\section{Results and Analysis}

We evaluate four optimization strategies on two continuous control benchmarks:
Ant-v3 (stationary, 100M steps) and SlipperyAnt-v3 (non-stationary with changing
friction, 20M steps). The methods compared are: Standard PPO, PPO with L2
regularization (L2), PPO with No-Shrink Adam optimizer (NS Adam), and
Continual Backpropagation (CBP).

\subsection{Performance and Plasticity Metrics}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{returns_ant.png}
    \includegraphics[width=0.48\textwidth]{returns_sant.png}
    \caption{Average episodic returns on Ant-v3 (left) and SlipperyAnt-v3 (right).
    Dashed vertical lines indicate friction changes in SlipperyAnt-v3.}
    \label{fig:returns}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{dormant_ant.png}
    \includegraphics[width=0.48\textwidth]{dormant_sant.png}
    \caption{Percentage of dormant neurons over training. A neuron is considered
    dormant if it fires on $\leq 1\%$ of observations within a 1000-step window.}
    \label{fig:dormant}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{stable_rank_ant.png}
    \includegraphics[width=0.48\textwidth]{stable_rank_sant.png}
    \caption{Stable rank of the policy network's feature representations.
    Higher stable rank indicates greater effective dimensionality and expressivity.}
    \label{fig:stable_rank}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{weight_magnitude_ant.png}
    \includegraphics[width=0.48\textwidth]{weight_magnitude_sant.png}
    \caption{Average weight magnitude of the policy network over training.}
    \label{fig:weights}
\end{figure}

Figures~\ref{fig:returns}--\ref{fig:weights} present the learning performance and
plasticity indicators across all methods. On Ant-v3, CBP achieves the highest
stable performance ($\sim$5000 return), followed by L2 ($\sim$4200). NS Adam
initially matches CBP but degrades significantly after 40M steps, settling around
3000. Standard PPO exhibits catastrophic collapse around 20M steps, with returns
dropping to near zero. On SlipperyAnt-v3, the non-stationary nature (friction
changes every 2M steps) reveals stark differences in adaptation capability---CBP
maintains robust performance throughout, while NS Adam and Standard PPO struggle
to adapt after major friction changes.

The dormant neuron counts (Figure~\ref{fig:dormant}) directly correlate with
performance degradation. NS Adam accumulates the most dormant neurons, reaching
$\sim$58\% on Ant-v3 and $\sim$42\% on SlipperyAnt-v3. Standard PPO shows high
variability (20--40\%) with unstable dynamics. CBP maintains consistently low
counts ($\sim$1--2\%), while L2 keeps them moderate ($\sim$3--5\%).

The stable rank metric (Figure~\ref{fig:stable_rank}) reveals network expressivity
loss. CBP maintains high rank ($\sim$210--220), preserving representation capacity.
NS Adam collapses from $\sim$240 to $\sim$30--50, indicating severe dimensionality
reduction. Weight dynamics (Figure~\ref{fig:weights}) show unbounded growth in
NS Adam and Standard PPO (0.08 $\rightarrow$ 0.12), while CBP maintains stability
($\sim$0.022) and L2 achieves the lowest magnitude ($\sim$0.007).

\begin{table}[h]
\centering
\caption{Summary of method performance across all metrics.}
\label{tab:summary}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Return} & \textbf{Dormant \%} & \textbf{Stable Rank} & \textbf{Weight Mag.} \\
\midrule
CBP & \textbf{Best} & \textbf{1--2\%} & \textbf{210--220} & Stable (0.022) \\
L2 & Good & 3--5\% & 160--180 & \textbf{Low (0.007)} \\
NS Adam & Degrading & 42--58\% & 30--50 & Growing (0.12) \\
Standard PPO & Collapsed & 20--40\% & Unstable & Growing (0.11) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Predictive Margin Analysis}

While dormant neuron counting provides a retrospective measure of plasticity loss,
we introduce a \textit{predictive margin} framework that can forecast neuron death
\textit{before} it occurs, enabling proactive intervention.

\subsubsection{Margin Definition}

For a ReLU neuron $i$ with weights $w_i$ and bias $b_i$, the \textit{margin} on a
set of observations $M$ is defined as:
\begin{equation}
    \text{Margin}_i(M) = \max_{x \in M} (w_i^\top x + b_i)
    \label{eq:margin}
\end{equation}

This margin represents the maximum pre-activation value across all observations.
A neuron is classified as:
\begin{itemize}
    \item \textbf{Dead}: $\text{Margin}_i < 0$ (never activates on any observation)
    \item \textbf{At-risk}: $0 \leq \text{Margin}_i < \tau$ (activates rarely, threshold $\tau$)
    \item \textbf{Healthy}: $\text{Margin}_i \geq \tau$ (activates robustly)
\end{itemize}

\subsubsection{Predictive vs. Actual Margin}

We distinguish between two margin computations at each PPO update $k$:

\begin{itemize}
    \item \textbf{Actual Margin}: Computed with current weights $\theta_k$ on current
    data $M_k$. This measures which neurons are dead \textit{after} training.
    \item \textbf{Predictive Margin}: Computed with \textit{previous} weights
    $\theta_{k-1}$ on current data $M_k$. This forecasts which neurons \textit{will}
    struggle with the new data distribution before training occurs.
\end{itemize}

The predictive margin answers a critical question: ``Given the weights from the
previous update, how many neurons would fail to activate on the incoming batch?''
This enables early warning of plasticity loss.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \draw[->, thick] (0,0) -- (12,0) node[right] {Time};

        % Update k-1
        \draw[thick] (1,0.2) -- (1,-0.2) node[below] {Update $k{-}1$};
        \node[above] at (1,0.3) {$\theta_{k-1}$};

        % Data collection
        \draw[<->, thick, blue] (1.5,0.5) -- (5.5,0.5);
        \node[above, blue] at (3.5,0.5) {Collect $M_k$};

        % Update k
        \draw[thick] (6,0.2) -- (6,-0.2) node[below] {Update $k$};
        \node[above] at (6,0.3) {$\theta_k$};

        % Margins
        \draw[->, thick, red] (5.7,-0.8) -- (3,-0.8) node[left] {Predictive: $\theta_{k-1}$ on $M_k$};
        \draw[->, thick, green!50!black] (6.3,-1.3) -- (8,-1.3) node[right] {Actual: $\theta_k$ on $M_k$};
    \end{tikzpicture}
    \caption{Timeline of predictive vs. actual margin computation. Predictive margin
    uses weights from before training on the current batch.}
    \label{fig:margin_timeline}
\end{figure}

\subsubsection{Validation: Correlation with Dormant Units}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{dormant_vs_margin_predictive_ant.png}
    \includegraphics[width=0.48\textwidth]{dormant_vs_margin_predictive_sant.png}
    \caption{Comparison of dormant units (blue, activity-based) vs. predictive margin
    dead percentage (red, geometry-based) on Ant-v3 (left) and SlipperyAnt-v3 (right).
    Shaded regions indicate rolling standard deviation.}
    \label{fig:margin_comparison}
\end{figure}

Figure~\ref{fig:margin_comparison} validates the margin-based approach by comparing
it against the established dormant unit metric. Despite measuring plasticity through
fundamentally different mechanisms---activity-based counting vs. geometric analysis
of pre-activations---both metrics exhibit strong correlation across all methods
and environments.

Key observations:
\begin{itemize}
    \item \textbf{Strong correlation}: The predictive margin dead percentage tracks
    the dormant unit count closely, validating that both capture the same underlying
    phenomenon of neuron death.
    \item \textbf{Consistent across methods}: The correlation holds for all four
    optimization strategies, from well-behaved (CBP, L2) to pathological cases
    (NS Adam, Standard PPO).
    \item \textbf{Non-stationary robustness}: On SlipperyAnt-v3, both metrics
    respond similarly to friction changes, showing increased dead neurons during
    low-friction periods and partial recovery during high-friction phases.
\end{itemize}

\subsubsection{Advantages of Predictive Margin}

The predictive margin framework offers several advantages over traditional
dormant unit counting:

\begin{enumerate}
    \item \textbf{Predictive capability}: By using $\theta_{k-1}$ on $M_k$, we can
    identify at-risk neurons \textit{before} training, enabling preemptive
    reinitialization strategies.

    \item \textbf{Geometric interpretation}: The margin provides a continuous
    measure of ``health''---neurons with small positive margins are at risk,
    while those with large margins are robust.

    \item \textbf{Per-update granularity}: Margins can be computed at each PPO
    update (every 2048 steps), providing fine-grained monitoring without requiring
    separate activity tracking buffers.

    \item \textbf{Intervention threshold}: The margin value naturally suggests
    intervention thresholds---neurons below a margin threshold $\tau$ could be
    candidates for reinitialization even if not yet fully dead.
\end{enumerate}

\subsubsection{Implications for Plasticity Maintenance}

The strong correlation between predictive margin and dormant units suggests that
margin-based monitoring could be integrated into plasticity-preserving algorithms:

\begin{equation}
    \mathcal{R}_k = \{i : \text{Margin}_i(\theta_{k-1}, M_k) < \tau\}
    \label{eq:reinit_set}
\end{equation}

where $\mathcal{R}_k$ is the set of neurons to reinitialize at update $k$. This
approach would allow CBP-style selective reinitialization to be triggered based
on geometric criteria rather than activity counts, potentially providing earlier
intervention and more principled threshold selection.

\section{Conclusion}

This study provides empirical evidence that loss of plasticity is a critical
failure mode in long-horizon reinforcement learning, and that maintaining network
plasticity is essential for sustained learning and adaptation.

Our experiments on Ant-v3 (100M steps) and SlipperyAnt-v3 (20M steps with
non-stationary dynamics) reveal a consistent pattern: methods that fail to
preserve plasticity---as measured by dormant neuron accumulation, stable rank
collapse, and unbounded weight growth---exhibit degraded or catastrophic
performance. Standard PPO completely collapses on Ant-v3, while NS Adam shows
progressive degradation despite initially strong performance. In contrast,
Continual Backpropagation (CBP) maintains robust performance throughout training
by keeping dormant neurons below 2\%, preserving high stable rank ($\sim$210),
and controlling weight magnitudes.

The non-stationary SlipperyAnt-v3 environment provides particularly compelling
evidence for the importance of plasticity. When friction changes every 2M steps,
methods with high dormant neuron counts struggle to adapt to new dynamics, while
CBP demonstrates consistent recovery. This suggests that plasticity is not merely
important for initial learning, but is critical for continual adaptation in
changing environments---a common requirement in real-world applications.

We introduced a predictive margin framework that provides a geometry-based
alternative to activity-based dormant neuron counting. The strong correlation
between predictive margin dead percentage and dormant units validates this
approach as a reliable plasticity indicator. Importantly, the predictive margin
offers a unique advantage: by evaluating previous weights on current data, it
can forecast neuron death \textit{before} training occurs, enabling proactive
intervention strategies.

These findings have practical implications for deep reinforcement learning:
\begin{enumerate}
    \item \textbf{Plasticity monitoring is essential}: Long-horizon RL training
    should include plasticity metrics (dormant units, stable rank, or margins)
    as standard diagnostics alongside reward curves.

    \item \textbf{Intervention mechanisms matter}: Methods like CBP that actively
    maintain plasticity through selective reinitialization significantly outperform
    passive approaches (Standard PPO) and partially effective regularization (L2).

    \item \textbf{Predictive metrics enable proactive strategies}: The margin-based
    framework opens possibilities for anticipatory plasticity maintenance, potentially
    improving upon reactive approaches that only intervene after neurons die.
\end{enumerate}

Future work should explore margin-based reinitialization triggers, investigate
the interplay between plasticity loss and catastrophic forgetting in continual
RL settings, and extend these analyses to more complex environments and network
architectures.

\section*{References}
\small

[1] Lyle, C., Rowland, M., \& Dabney, W.\ (2022) Understanding and preventing
capacity loss in reinforcement learning. In {\it International Conference on
Learning Representations (ICLR)}.

[2] Dohare, S., Sutton, R.S., \& Mahmood, A.R.\ (2024) Loss of plasticity in
deep continual learning. {\it Nature} {\bf 632}:768--774.

[3] Dohare, S., Hernandez-Garcia, J.F., Lan, Q., Rahman, P., Mahmood, A.R., \&
Sutton, R.S.\ (2024) Maintaining plasticity in deep continual learning. {\it
Nature Machine Intelligence} {\bf 6}:296--308.

[4] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., \& Klimov, O.\ (2017)
Proximal policy optimization algorithms. {\it arXiv preprint arXiv:1707.06347}.

[5] Sokar, G., Agarwal, R., Castro, P.S., \& Evci, U.\ (2023) The dormant neuron
phenomenon in deep reinforcement learning. In {\it International Conference on
Machine Learning (ICML)}, pp.\ 32145--32168. PMLR.

[6] Kumar, A., Agarwal, R., Ghosh, D., \& Levine, S.\ (2022) Implicit under-
parameterization inhibits data-efficient deep reinforcement learning. In {\it
International Conference on Learning Representations (ICLR)}.

[7] Nikishin, E., Schwarzer, M., D'Oro, P., Bacon, P.-L., \& Courville, A.\
(2022) The primacy bias in deep reinforcement learning. In {\it International
Conference on Machine Learning (ICML)}, pp.\ 16828--16847. PMLR.

[8] Abbas, Z., Zhao, R., Modayil, J., White, A., \& Machado, M.C.\ (2023) Loss
of plasticity in continual deep reinforcement learning. In {\it Conference on
Lifelong Learning Agents (CoLLAs)}.

[9] Todorov, E., Erez, T., \& Tassa, Y.\ (2012) MuJoCo: A physics engine for
model-based control. In {\it IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS)}, pp.\ 5026--5033.

[10] Lu, C., Ball, P.J., Rudner, T.G.J., Parker-Holder, J., Osborne, M.A., \&
Teh, Y.W.\ (2023) Synthetic experience replay. In {\it Advances in Neural
Information Processing Systems (NeurIPS)} {\bf 36}.
```

---

## References

- `12102025.md` - Detailed discussion on margin analysis theory
- `12112025.md` - Predictive margin protocol for stationary RL
- `run_ppo_wandb.py` - Main training script with margin analysis
- `plot_all_metrics_v2.py` - Plotting script created today
