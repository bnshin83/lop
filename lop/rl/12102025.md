# Discussion Notes: Loss Curvature, Spectral Collapse, and Dead Neurons
**Date: December 10, 2025**

---

## 1. Parameter Space vs Input Space Curvature

### Question: In He 2025, is the loss curvature measured in input space or parameter space?

**Answer: Parameter space curvature**

The paper analyzes the **Hessian of the loss function with respect to the parameters** (âˆ‡Â²â„’ with respect to Î¸).

**Key evidence:**
- The eigenvalues measure "second-order curvature of the loss in the direction of the eigenvector váµ¢" â€” these eigenvectors are in parameter space
- The Hessian decomposition: **H = Jáµ€ âˆ‡Â²â„“ J + R** (Gauss-Newton decomposition), where J is the Jacobian of network output w.r.t. parameters
- They use the **generalized Gauss-Newton matrix H_GGN**, a parameter-space curvature measure
- Theorem 6.2 bounds Hessian rank by **P - k(I+O+1)**, where P is the "full parameter dimension"
- Ï„-trainability is defined based on "Hessian rank at task Ï„ initialization" â€” measuring independent directions in parameter space with non-zero curvature

The "spectral collapse" describes the collapse of the eigenspectrum of **âˆ‡Â²_Î¸ â„’** â€” the loss landscape becomes flat in most parameter directions.

---

## 2. Comparison with Garg et al. 2023 (Input Curvature for Memorization)

### Question: Garg estimated input curvature with trace of input loss Hessian. How does this relate to H_GGN?

**Garg et al. compute input-space curvature:**

They explicitly define:
> H(X) âˆˆ â„^(dÃ—d), where [H(X)]áµ¢,â±¼ = âˆ‚Â²L(X) / âˆ‚xáµ¢âˆ‚xâ±¼

This is the **Hessian with respect to the input X**, not the parameters. They use Tr(HÂ²) = Î£áµ¢ Î»áµ¢Â² to measure the curvature magnitude around each data sample.

### The Key Distinction

In He 2025, the Hessian decomposes as:

**H = Jáµ€ âˆ‡Â²â„“ J + R**

Where:
- **âˆ‡Â²â„“** (or **H_z**) is the Hessian of the loss with respect to the **network output z** (pre-softmax logits)
- **J** is the Jacobian âˆ‚z/âˆ‚Î¸ (output w.r.t. parameters)
- **R** is the residual term

The term **âˆ‡Â²â„“** is an "output-space" curvature â€” it measures how the loss curves with respect to the logits z = f(x; Î¸).

### Summary Table

| Paper | Hessian | Space |
|-------|---------|-------|
| Garg et al. | âˆ‚Â²L/âˆ‚xÂ² | **Input space** (pixel perturbations) |
| He 2025 (full H) | âˆ‚Â²L/âˆ‚Î¸Â² | **Parameter space** |
| He 2025 (âˆ‡Â²â„“ term) | âˆ‚Â²L/âˆ‚zÂ² | **Output/logit space** |

**H_GGN = Jáµ€ âˆ‡Â²â„“ J** transforms output-space curvature (âˆ‡Â²â„“) into parameter space via the Jacobian.

The relationships through chain rule:
- Garg's input Hessian: How does loss change as you perturb **x**?
- He's âˆ‡Â²â„“: How does loss change as you perturb **z** (network output)?
- He's full H or H_GGN: How does loss change as you perturb **Î¸**?

---

## 3. The Effective Rank Penalty in He 2025

### The Core Objective

min_Î¸ g_Ï„(Î¸) = ğ”¼[â„“_Ï„(F_Î¸(x), y)] âˆ’ erank(Î£â‚— ğ”¼â‚™[xâ‚™Ë¡ xâ‚™Ë¡áµ€]) + Î»â€–Î¸â€–Â²

The middle term is the **effective rank of the layer-wise input covariance matrices**.

### What is xâ‚™Ë¡?

**xâ‚™Ë¡ âˆˆ â„^(M_{l-1})** is the **input (activation) to layer l** for sample n â€” the output of layer lâˆ’1 after activation. This is **not** the raw input x to the network, but the **intermediate representation** at each layer.

So **xâ‚™Ë¡ xâ‚™Ë¡áµ€** is the **outer product** of layer l's input with itself, and **ğ”¼â‚™[xâ‚™Ë¡ xâ‚™Ë¡áµ€]** is the **input covariance matrix** for that layer.

### The Theoretical Chain: Input Covariance â†’ Parameter-Space Curvature

**Step 1: Hessian Decomposition**

âˆ‡Â²_Î¸ â„’ = H_GGN + R

**Step 2: KFAC Factorization of H_GGN**

For a single layer l:

H_GGN = A Î£â‚™ Î£â‚’ (xâ‚™Ë¡ xâ‚™Ë¡áµ€) âŠ— (gâ‚™,â‚’Ë¡ gâ‚™,â‚’Ë¡áµ€)

This is a **Kronecker product** of:
- **xâ‚™Ë¡ xâ‚™Ë¡áµ€** â€” input statistics (activation covariance)
- **gâ‚™,â‚’Ë¡ gâ‚™,â‚’Ë¡áµ€** â€” backpropagated gradient statistics

**Step 3: KFAC Approximation**

Under independence assumptions:

H_GGN â‰ˆ (ğ”¼â‚™[xâ‚™Ë¡ xâ‚™Ë¡áµ€]) âŠ— (ğ”¼â‚™[gâ‚™Ë¡ gâ‚™Ë¡áµ€])

**Step 4: Rank Property of Kronecker Products**

rank(A âŠ— B) = rank(A) Ã— rank(B)

Therefore:

**rank(H_GGN) âˆ rank(ğ”¼[xâ‚™Ë¡ xâ‚™Ë¡áµ€]) Ã— rank(ğ”¼[gâ‚™Ë¡ gâ‚™Ë¡áµ€])**

### Why Maximize Input Covariance Rank?

> "The rank of the input covariance matrix ğ”¼â‚™[xâ‚™Ë¡ xâ‚™Ë¡áµ€] at layer l is the same as the rank of the input representation of layer l itself"

**Low-rank activations â†’ Low-rank H_GGN â†’ Spectral collapse â†’ Loss of plasticity**

### Connection to Garg's Input Curvature

| Aspect | Garg et al. 2023 | He et al. 2025 |
|--------|------------------|----------------|
| **What is measured** | âˆ‚Â²L/âˆ‚xÂ² (Hessian w.r.t. raw input) | ğ”¼[xË¡ xË¡áµ€] (activation covariance per layer) |
| **Space** | Input pixel space | Layer-wise activation space |
| **Purpose** | Measure memorization of individual samples | Maintain parameter-space curvature for trainability |
| **Computation** | Tr(HÂ²) via finite differences | Effective rank via SVD of stacked features |

### The Role of L2 Regularization

L2 is needed because maximizing H_GGN rank doesn't account for the residual R. Theorem B.3 shows that with appropriate Î»:

**â†‘ rank(H_GGN) âŸ¹ â†‘ rank(H)**

### Summary

The effective rank penalty operates on **layer-wise activation covariance matrices** (xâ‚™Ë¡ xâ‚™Ë¡áµ€), which are:
1. **Not** the input-space Hessian (like Garg)
2. **Not** the parameter-space Hessian directly
3. Rather, a **factor** that determines parameter-space curvature through KFAC decomposition

The chain:

**Activation diversity (erank of xË¡)** â†’ **Input covariance rank** â†’ **H_GGN rank** â†’ **Full Hessian rank** â†’ **Ï„-trainability**

---

## 4. Causal Direction: Dead Neurons vs Spectral Collapse

### Question: Does spectral collapse cause dead neurons, or do dead neurons cause spectral collapse?

**Answer: Dead neurons cause spectral collapse** (primarily)

### Evidence from Theorem 6.2

> "The following theorem **upper bounds the Hessian rank** of a task Ï„, **in terms of the number of dead neurons**"

rank(H_Ï„^(0)) â‰¤ P âˆ’ k_Ï„(I + O + 1)

Where k_Ï„ is the number of dead neurons. This is a **one-directional bound**: dead neurons â†’ reduced Hessian rank.

### The Mechanistic Pathway

A neuron is Ï„-dead when:

Ïƒ(W_{[j,:],l}áµ€ F_{l-1}(x) + b) = 0 for all x âˆˆ X_Ï„

When a neuron is dead:
- Its output xâ±¼Ë¡ = 0 for all samples
- The j-th row/column of **ğ”¼[xË¡ xË¡áµ€]** becomes zero
- This directly **removes one rank** from input covariance
- Via KFAC: rank(H_GGN) drops

**Causal chain:**

Dead neuron â†’ Zero activation row â†’ Reduced rank(ğ”¼[xË¡ xË¡áµ€]) â†’ Reduced rank(H_GGN) â†’ Spectral collapse

### The Feedback Loop

There's a **reinforcing feedback loop**:

```
Dead neurons â†’ Spectral collapse â†’ Fewer trainable directions
     â†‘                                        â†“
     â””â”€â”€â”€â”€ More neurons die â†â”€â”€ Poor optimization â†â”˜
```

Both pathways lead to the same outcome:
1. **Prevent neurons from dying** â†’ maintains rank
2. **Directly maximize rank** â†’ keeps neurons useful/active

### Why Dead Neurons â†’ Collapse (Not Reverse)

**Spectral collapse cannot cause dead neurons directly** because:
- A neuron dies when pre-activation is negative for all inputs: **wáµ€x + b < 0**
- This is a property of **weights and data**, not the Hessian spectrum
- The Hessian spectrum describes **loss landscape curvature**, which affects optimization but doesn't directly set neuron outputs

However, **spectral collapse can indirectly cause more dead neurons** through:
- Poor optimization â†’ weights drift to bad regions
- Fewer gradient directions â†’ inability to "rescue" dying neurons

---

## 5. The Missing Question: Why Do Neurons Die?

### What the Paper Does NOT Explain

- **Why neurons die in the first place**
- The root cause that initiates the cascade

### Hypotheses in the Literature

| Hypothesis | Mechanism | Evidence |
|------------|-----------|----------|
| **Large negative bias drift** | SGD updates accumulate negative bias | Observed empirically |
| **Weight norm explosion/collapse** | Weights grow unbounded or shrink | L2 regularization helps |
| **Distribution shift** | New task data lies in previously negative region | Continual learning specific |
| **Gradient starvation** | Dead neurons get zero gradient â†’ can't recover | Self-reinforcing |
| **Initialization sensitivity** | Some neurons start near death boundary | Random seed effects |
| **Feature co-adaptation** | Neurons become redundant, some get pruned | Dropout literature |

### Continual Learning Specific Problem

A neuron useful for task Ï„â‚ might become dead for task Ï„â‚‚ because:
**The data distribution shifts, but weights were optimized for the old distribution**

### The Missing Causal Story

```
???
 â†“
Neurons die
 â†“
Spectral collapse
 â†“
Loss of plasticity
```

The paper enters the chain at "neurons die" without explaining the "???".

### What Would Answer This Question

1. **Trajectory analysis**: Track individual neuron pre-activations throughout training
2. **Gradient decomposition**: What gradient signals push neurons toward death?
3. **Counterfactual experiments**: If we artificially prevent a neuron from dying, what happens?
4. **Task-specific analysis**: Do certain task sequences kill neurons faster?

---

## 6. Geometric Interpretation of Dead Neurons

### A Single Neuron as a Hyperplane

For neuron with weights **w** âˆˆ â„áµˆ and bias b:

**Pre-activation: z = wáµ€x + b**

The equation **wáµ€x + b = 0** defines a **hyperplane** in input space:

```
                    wáµ€x + b > 0  (neuron fires)
                         â†‘
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† Hyperplane
                         â†“
                    wáµ€x + b < 0  (neuron dead)
```

### Data Manifold

Real data lies on a lower-dimensional **data manifold** M âŠ‚ â„áµˆ:

```
        â„áµˆ (ambient space)

           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚    ~~~~~~~~~~~~  â”‚  â† Data manifold M
           â”‚   ~            ~ â”‚    (curved surface)
           â”‚  ~    â€¢  â€¢  â€¢  ~ â”‚    â€¢ = data points
           â”‚   ~  â€¢    â€¢   ~  â”‚
           â”‚    ~~~~~~~~~~    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dead Neuron: Hyperplane Misses the Manifold

A neuron is **dead** when its hyperplane lies entirely on one side of the data manifold:

```
    Hyperplane
        â”‚
        â”‚         ~~~~~~~~~~~~
        â”‚        ~            ~
        â”‚       ~   Data       ~
        â”‚      ~   Manifold    ~
        â”‚       ~            ~
        â”‚        ~~~~~~~~~~~~
        â”‚
        â”‚   wáµ€x + b < 0 for ALL x âˆˆ M
```

**Geometrically**: The hyperplane doesn't intersect the data manifold.

### Layer-wise Transformation

Each layer transforms the manifold:

```
Layer 0 (input)     Layer 1            Layer 2            Layer 3

   ~~~~~~           ~~~~               ~~~                ~~
  ~      ~    â†’    ~    ~      â†’      ~   ~      â†’       ~  ~
 ~   Mâ‚€   ~       ~  Mâ‚  ~           ~ Mâ‚‚ ~             ~Mâ‚ƒ~
  ~      ~         ~    ~             ~   ~              ~  ~
   ~~~~~~           ~~~~               ~~~                ~~
```

As representations pass through layers:
- The manifold can **stretch, compress, fold**
- Dead neurons **project out dimensions** (zeroing them)
- Eventually the manifold can **collapse** to lower dimension

### Continual Learning: Manifold Shift

Each task has a different data manifold:

```
Task Ï„â‚:                    Task Ï„â‚‚:

    Hyperplane                  Hyperplane (same weights!)
        â”‚                           â”‚
        â”‚    ~~~~~~                 â”‚
        â”‚   ~      ~                â”‚         ~~~~~~
        â”‚  ~  M_Ï„â‚  ~               â”‚        ~      ~
        â”‚   ~      ~                â”‚       ~  M_Ï„â‚‚  ~
        â”‚    ~~~~~~                 â”‚        ~      ~
        â”‚                           â”‚         ~~~~~~
        â†“                           â†“
  (Intersects M_Ï„â‚)          (Misses M_Ï„â‚‚ entirely!)
   Neuron ALIVE               Neuron DEAD for Ï„â‚‚
```

**The hyperplane was learned for Ï„â‚'s manifold, but Ï„â‚‚'s manifold lies elsewhere.**

### Why This Causes Rank Collapse

Each **active** neuron contributes a dimension:

```
    Neuron 1: wâ‚áµ€x + bâ‚ â†’ ReLU â†’ yâ‚  (active)
    Neuron 2: wâ‚‚áµ€x + bâ‚‚ â†’ ReLU â†’ yâ‚‚  (active)
    Neuron 3: wâ‚ƒáµ€x + bâ‚ƒ â†’ ReLU â†’ 0   â† DEAD
    Neuron 4: wâ‚„áµ€x + bâ‚„ â†’ ReLU â†’ yâ‚„  (active)
    Neuron 5: wâ‚…áµ€x + bâ‚… â†’ ReLU â†’ 0   â† DEAD

Output: effectively â„Â³ (only 3 non-zero dims)
```

The output covariance matrix **ğ”¼[yË¡ yË¡áµ€]** has:
- Zero rows/columns for dead neurons
- Rank â‰¤ (number of active neurons)

### Visual Summary

```
                        HEALTHY                              DYING

    Data Manifold:      ~~~~~~~~                            ~~~~~~~~
                       ~        ~                          ~        ~
    Hyperplanes:      â”€â”€/â”€â”€\â”€â”€/â”€â”€\â”€â”€                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                     (intersect manifold)                (all miss manifold)

    Activations:      [0.3, 0.0, 0.8, 0.2, 0.5]          [0.0, 0.0, 0.0, 0.0, 0.1]
                      (diverse, high rank)                (sparse, low rank)

    Covariance:       Full rank matrix                    Near-zero matrix

    Hessian:          Many curvature directions           Few curvature directions

    Learning:         Can adapt to new tasks              STUCK
```

### Key Geometric Insight

**A dead neuron means its decision hyperplane has drifted away from the data manifold.**

In continual learning:
1. The manifold shifts (new task)
2. The hyperplane stays fixed (weights from old task)
3. The new manifold lies entirely in the "off" half-space

The network progressively loses its ability to "sense" the data as more hyperplanes drift away from where the data lives.

---

## 7. Experimental Strategies to Prove Dead Neurons Cause Plasticity Loss

### Overview

The intuition is clear for continual learning where manifolds shift. But proving it experimentally requires careful design.

### Experiment 1: Direct Causal Intervention â€” Prevent Neurons from Dying

**Idea**: If dead neurons cause plasticity loss, artificially keeping neurons alive should preserve plasticity.

| Intervention | Method | Expected Result |
|--------------|--------|-----------------|
| **Leaky ReLU** | Replace ReLU with LeakyReLU (never fully dead) | Reduced plasticity loss |
| **Neuron resurrection** | Periodically reinitialize dead neurons | Restored plasticity |
| **Bias clipping** | Prevent bias from becoming too negative | Fewer dead neurons |
| **Activation clamping** | Force minimum activation > 0 | No dead neurons |

**Control**: Same architecture, same tasks, only change the intervention.

**Measurement**:
- Count dead neurons over tasks
- Track effective rank over tasks
- Track accuracy on new tasks (plasticity)

```
If: Intervention â†’ Fewer dead neurons â†’ Higher rank â†’ Better plasticity
Then: Causal link confirmed
```

### Experiment 2: Same Task Repeated (No Manifold Shift Control)

**Critical control**: What happens with NO manifold shift?

**Experiment**:
```
Condition A: Ï„â‚ â†’ Ï„â‚‚ â†’ Ï„â‚ƒ â†’ ... (different tasks, manifold shifts)
Condition B: Ï„â‚ â†’ Ï„â‚ â†’ Ï„â‚ â†’ ... (same task repeated, no shift)
```

**Prediction**:
- Condition A: Dead neurons accumulate, plasticity lost
- Condition B: No additional dead neurons, plasticity maintained

**If Condition B shows no dead neurons**: Confirms manifold shift is the cause

### Experiment 3: Layer-wise Margin Analysis for Neuron Death

**Idea**: Instead of predicting neuron death (which is infeasible due to layer-wise dependencies), perform **retrospective geometric analysis** using pre-activation margins.

**Why prediction is infeasible**:
- For layer 1, input x is fixed (raw data) â€” prediction is straightforward
- For layer 2+, input depends on which neurons are dead in previous layers
- This creates cascading dependencies â€” a chicken-and-egg problem
- Additionally, weight updates during training can rescue or kill neurons

**Feasible alternative**: Margin-based retrospective analysis

#### Death Condition (Geometric)

A neuron j is **dead** when its hyperplane misses the data manifold entirely:

```
max_{x âˆˆ M} (wâ±¼áµ€x + bâ±¼) < 0  â†’  Neuron is DEAD
```

**Logic**: If even the maximum pre-activation is negative, the neuron never fires.

#### Full Characterization

| Condition | Meaning |
|-----------|---------|
| **max < 0** | All pre-activations negative â†’ **DEAD** |
| **min > 0** | All pre-activations positive â†’ **ALWAYS ON** |
| **min < 0 < max** | Some positive, some negative â†’ **SELECTIVE** (healthy) |

#### Protocol: Sequential Layer-wise Margin Computation

```python
def compute_margins_per_layer(network, task2_data):
    """
    Compute death-risk margins for each layer sequentially.
    Uses MAX because: max < 0 means neuron NEVER fires (dead).
    """
    margins = {}
    x = task2_data  # Start with raw input

    for l, layer in enumerate(network.layers):
        W, b = layer.weight, layer.bias
        pre_act = x @ W.T + b  # Shape: (N_samples, N_neurons)

        # MAX: highest pre-activation across all samples
        # If max < 0, neuron is dead (never fires)
        margins[l] = pre_act.max(dim=0)  # Shape: (N_neurons,)

        # Propagate to next layer (with ReLU)
        x = ReLU(pre_act)

    return margins

# Interpretation:
# margin < 0  â†’  Dead (never fires)
# margin â‰ˆ 0  â†’  At risk (barely fires on few samples)
# margin > 0  â†’  Alive (fires on at least some inputs)
```

#### Metrics to Compute

```python
for each task Ï„:
    for each layer l:
        1. margin_distribution[Ï„][l] = all neuron margins (using max)
        2. dead_count[Ï„][l] = count(margin < 0)
        3. at_risk_count[Ï„][l] = count(0 < margin < Îµ)
        4. mean_margin[Ï„][l] = average margin

    # Also compute effective rank for comparison
    5. effective_rank[Ï„][l] = erank(activation_covariance)
```

#### Analysis

1. **Death accumulation**: Plot dead_count[Ï„][l] over tasks Ï„ and layers l
2. **Margin erosion**: Plot mean_margin[Ï„][l] over tasks â€” does it decrease?
3. **Layer-wise death profile**: Where do neurons die most?
   ```
   Layer 1: 5% dead
   Layer 2: 12% dead
   Layer 3: 25% dead  â† Death accumulates through layers?
   Layer 4: 40% dead
   ```
4. **Correlation**: margin vs effective_rank vs accuracy
5. **ROC analysis**: Can margin predict death? What threshold separates dead from alive?

#### Expected Results

```
Pre-activation margins across neurons:

DEAD neuron:      max = -0.5 < 0   âœ— (never fires)
AT-RISK neuron:   max = +0.2 â‰ˆ 0   âš  (barely fires)
ALIVE neuron:     max = +2.1 > 0   âœ“ (fires strongly on some samples)
```

#### Interpretability Note

- **Layer 1**: High interpretability (raw data geometry)
- **Layer 2-3**: Medium interpretability
- **Deep layers**: Lower interpretability (complex manifold transformations)

Focus analysis on **early-to-middle layers** where geometry is more interpretable.

### Experiment 4: Dose-Response (Manifold Distance)

**Idea**: Vary the degree of manifold shift and measure response.

**Independent variable**: Distance between task manifolds
```
d(M_Ï„â‚, M_Ï„â‚‚) = small, medium, large
```

**Dependent variables**:
- Number of neurons that die
- Effective rank drop
- Accuracy on new task

**Expected result**:
```
Manifold distance â†‘ â†’ Dead neurons â†‘ â†’ Rank â†“ â†’ Plasticity â†“
```

### Experiment 5: Ablation â€” Artificially Kill Neurons

**Idea**: If dead neurons cause plasticity loss, artificially killing neurons should reproduce the effect.

**Protocol**:
1. Train network on Task 1 (healthy)
2. **Manually set k neurons to dead** (set their output to 0)
3. Train on Task 2
4. Measure plasticity

**Vary k**: 0%, 10%, 25%, 50%, 75% of neurons killed

**Expected result**:
```
More artificially killed neurons â†’ Lower effective rank â†’ Worse plasticity
```

### Experiment 6: Temporal Ordering (Granger Causality)

**Key experiment**: Show that dead neurons **precede** spectral collapse, not vice versa.

**Protocol**:
```
For each task Ï„:
    1. Measure dead neurons at START of task
    2. Measure Hessian rank at START of task
    3. Train on task
    4. Measure accuracy achieved
    5. Measure dead neurons at END of task
    6. Measure Hessian rank at END of task
```

**Analysis**:
- Granger causality: Do dead neurons at time t predict rank at time t+1?
- Cross-correlation: Which signal leads?
- Event study: When a neuron dies, what happens to rank in subsequent steps?

### Experiment 7: Track Individual Neuron Trajectories

**Fine-grained analysis**: Follow individual neurons through training.

**For each neuron j, track**:
- Pre-activation margin: max_x (wâ±¼áµ€x + bâ±¼) over current task data
- When does it cross zero? (death event)
- What was the gradient signal before death?
- Does it ever recover?

### Summary: Key Comparisons for Causality

| Experiment | What it shows | Difficulty |
|------------|---------------|------------|
| **Leaky ReLU intervention** | Dead neurons necessary for plasticity loss | Easy |
| **Same-task control** | Manifold shift necessary for neuron death | Easy |
| **Layer-wise margin analysis** | Geometric theory correct, retrospective validation | Medium |
| **Dose-response** | Quantitative relationship | Medium |
| **Artificial neuron killing** | Dead neurons sufficient for plasticity loss | Medium |
| **Temporal ordering** | Dead neurons precede rank collapse | Medium |
| **Neuron trajectory tracking** | Mechanism of death | Hard |

To establish causality:
1. **Necessary**: Prevent dead neurons â†’ Plasticity preserved
2. **Sufficient**: Artificially kill neurons â†’ Plasticity lost
3. **Temporal**: Dead neurons happen before rank collapse
4. **Dose-response**: More dead neurons â†’ More plasticity loss
5. **Mechanism**: Manifold shift â†’ Neuron death (validated geometrically)

---

## 8. Margin Analysis in Stationary RL Environments

### The Question: Does Margin Analysis Make Sense Without Task Changes?

In continual learning (e.g., SlipperyAnt with friction changes), margin analysis captures:
- **Task START**: Can old network handle new distribution?
- **Task END**: How many neurons died during adaptation?

But in **stationary environments** (e.g., Ant-v3 with fixed dynamics), there's no explicit task change. What does margin analysis measure?

### Key Insight: RL is Inherently Non-stationary

Even with fixed environment dynamics, the **learning problem itself is non-stationary**:

| What Changes | Why | Impact on Data Distribution |
|--------------|-----|---------------------------|
| **Policy Ï€(a\|s)** | Policy gradient updates | Different actions â†’ different state transitions |
| **State distribution d^Ï€(s)** | Better policy visits different states | Manifold shifts in state space |
| **Value targets V^Ï€(s)** | Better policy â†’ higher returns | Regression targets change |
| **Advantage estimates A(s,a)** | Value function improves | Optimization landscape changes |

```
Early training:   Ï€_random  â†’ visits states S_early  â†’ low returns
Mid training:     Ï€_medium  â†’ visits states S_mid    â†’ medium returns
Late training:    Ï€_skilled â†’ visits states S_late   â†’ high returns

S_early â‰  S_mid â‰  S_late  (implicit distribution shift!)
```

### The Implicit Manifold Shift in RL

```
Time â†’
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º

SlipperyAnt:   [Task 1: friction=0.5]â”€â”€[Task 2: friction=1.2]â”€â”€[Task 3]
               M_Ï„â‚ (explicit)          M_Ï„â‚‚ (explicit)         M_Ï„â‚ƒ
               â†‘ External manifold shift (friction change)

Ant-v3:        [â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€]
               Ï€â‚ â†’ Ï€â‚‚ â†’ Ï€â‚ƒ â†’ Ï€â‚„ â†’ ... â†’ Ï€_final
               M_Ï€â‚ â†’ M_Ï€â‚‚ â†’ M_Ï€â‚ƒ â†’ M_Ï€â‚„ â†’ ... â†’ M_Ï€_final
               â†‘ Internal manifold shift (policy improvement)
```

**Both environments have distribution shift â€” the difference is the source:**
- SlipperyAnt: External (environment dynamics change)
- Ant-v3: Internal (agent's own policy improvement)

### What Margin Analysis Captures in Stationary RL

| Metric | Meaning in Stationary RL |
|--------|--------------------------|
| **Dead neurons** | Neurons that can't contribute to learning from **current** state distribution |
| **Increasing dead count** | Network losing capacity as state distribution shifts with policy improvement |
| **At-risk neurons** | Neurons close to dying as distribution continues to evolve |
| **Margin erosion** | Hyperplanes drifting away from where current policy visits |

### Geometric Interpretation

```
Early Training (random policy):     Late Training (skilled policy):

    Hyperplane                          Hyperplane (same neuron!)
        â”‚                                   â”‚
        â”‚    ~~~~~~                         â”‚
        â”‚   ~      ~                        â”‚    ~~~~~~
        â”‚  ~ States ~                       â”‚   ~      ~
        â”‚   ~ visited~                      â”‚  ~ States ~ (different region!)
        â”‚    ~~~~~~                         â”‚   ~ visited~
        â”‚                                   â”‚    ~~~~~~
        â†“                                   â†“
  (Intersects manifold)              (May miss new manifold!)
   Neuron ALIVE                       Neuron potentially DEAD
```

The hyperplane was useful for early training states, but as the policy improves and visits different states, the same hyperplane may no longer intersect the new state distribution.

### Why This Matters for Loss of Plasticity

In stationary RL, if neurons die as the policy improves:

1. **Early death** â†’ Network loses capacity to represent value function for better policies
2. **Mid-training death** â†’ Struggles to fine-tune as approaching optimal behavior
3. **Accumulated death** â†’ Performance plateaus despite continued training

```
Performance curve with plasticity loss:

Return â†‘
       â”‚                    â†â”€â”€ Plateau (dead neurons can't adapt)
       â”‚              â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
       â”‚         â—â—â—â—â—
       â”‚      â—â—â—
       â”‚    â—â—
       â”‚  â—â—
       â”‚â—â—
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Steps
         Early        Mid          Late
       (learning)  (slowing)    (stuck)
```

### Comparison: Explicit vs Implicit Non-stationarity

| Aspect | SlipperyAnt (Explicit) | Ant-v3 (Implicit) |
|--------|------------------------|-------------------|
| **Source of shift** | Environment dynamics change | Policy improvement |
| **When shift occurs** | Discrete (every 2M steps) | Continuous (every update) |
| **Magnitude** | Large (friction jumps) | Small but cumulative |
| **Margin analysis timing** | At task boundaries | Every update (continuous) |
| **Dead neuron cause** | New dynamics incompatible with old weights | New states incompatible with old hyperplanes |

### Experimental Protocol for Stationary RL

#### The Observation Buffer

The margin analysis uses a **circular buffer** that stores the last N observations (states) from the environment. This buffer represents "what states does the current policy visit?"

```
                    obs_buffer (size = 1000 Ã— 111 for Ant)
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  o_0   â”‚  o_1   â”‚  o_2   â”‚  ...  â”‚ o_998 â”‚ o_999  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
     obs_buffer_idx (wraps around when full)

Step 0:     [o_0,  empty, empty, ..., empty, empty]  idx=1
Step 1:     [o_0,  o_1,   empty, ..., empty, empty]  idx=2
...
Step 999:   [o_0,  o_1,   o_2,   ..., o_998, o_999]  idx=0  â† FULL
Step 1000:  [o_1000, o_1, o_2,   ..., o_998, o_999]  idx=1  â† Overwrites oldest
```

**Why use a buffer?**

| Purpose | Explanation |
|---------|-------------|
| **Represent current state distribution** | The 1000 observations approximate what states the current policy visits |
| **Compute margins** | Pass all 1000 observations through network to find dead neurons |
| **Rolling window** | Always contains the **most recent** 1000 observations |

**The key insight**: The buffer represents the current policy's state distribution:
- Early training: Buffer contains states from random policy
- Late training: Buffer contains states from skilled policy
- **These are different distributions** â†’ Margin analysis captures this implicit shift

#### Margin Computation Flow

```
INPUT:
  - network = pol.mean_net  (Policy network: [Linear, ReLU, Linear, ReLU, Linear])
  - data = obs_buffer       (1000 observations, shape: [1000, 111] for Ant)

PROCESS:
  x = obs_buffer                        # Shape: (1000, 111)

  FOR EACH HIDDEN LAYER l = 0, 1:       # 2 hidden layers for [256, 256]

      Layer l weights: W (256, input_dim), b (256,)

      pre_act = x @ W.T + b             # Shape: (1000, 256)

      For each neuron j (j = 0..255):
          max_pre_act[j] = max over 1000 samples of pre_act[:, j]

          if max_pre_act[j] < 0:
              â†’ Neuron j is DEAD (never fires on ANY of 1000 samples)
          elif max_pre_act[j] < 0.1:
              â†’ Neuron j is AT-RISK (barely fires)
          else:
              â†’ Neuron j is ALIVE

      x = ReLU(pre_act)                 # Propagate to next layer

OUTPUT:
  - dead_counts = {0: 5, 1: 12}         # Layer 0: 5 dead, Layer 1: 12 dead
  - at_risk_counts = {0: 8, 1: 15}      # Neurons close to dying
  - mean_margins = {0: tensor, 1: tensor}
```

#### Margin Logging Frequency

```python
# SlipperyAnt: Log at task boundaries
if task_changed:
    compute_margins()  # ~20 snapshots over 20M steps

# Ant-v3: Log every update (continuous tracking)
if step % batch_size == 0:  # Every 2048 steps
    compute_margins()  # ~48,000 snapshots over 100M steps
```

#### Timeline

```
Step:     0      2048    4096    6144    8192    ...    100M
          â”‚       â”‚       â”‚       â”‚       â”‚              â”‚
          â–¼       â–¼       â–¼       â–¼       â–¼              â–¼
Buffer:  [fill]  [full]  [full]  [full]  [full]        [full]
          â”‚       â”‚       â”‚       â”‚       â”‚              â”‚
Margin:   -      CALC    CALC    CALC    CALC    ...   CALC
                  â”‚       â”‚       â”‚       â”‚              â”‚
                  â–¼       â–¼       â–¼       â–¼              â–¼
              ~48,828 margin computations total (for Ant-v3)
```

#### Key Metrics to Track

```python
# Per-update metrics
margin_data = {
    'dead_counts': [],      # Per-layer dead neuron counts
    'at_risk_counts': [],   # Per-layer at-risk counts
    'mean_margins': [],     # Per-layer average margins
    'timestamps': [],       # Step numbers
    'update_numbers': [],   # PPO update counts
}

# Correlate with:
# - Episode returns (performance)
# - Effective rank (representation quality)
# - Weight magnitudes (optimization health)
```

#### Analysis Questions

1. **When do neurons start dying?** Early, mid, or late training?
2. **Death rate vs return**: Does death accelerate when returns plateau?
3. **Layer-wise patterns**: Which layers lose neurons first?
4. **Algorithm comparison**: Does CBP/L2 prevent death in stationary RL?

### Summary

| Question | Answer |
|----------|--------|
| Does margin analysis make sense in stationary RL? | **Yes** â€” RL is inherently non-stationary due to policy improvement |
| What causes the distribution shift? | Policy improvement â†’ different states visited |
| What do dead neurons indicate? | Loss of capacity to learn from current state distribution |
| How often to compute margins? | Every update (continuous tracking of gradual shift) |
| Expected finding? | Neurons die gradually as policy visits increasingly different states |

### The Unified View

```
                    CONTINUAL LEARNING              RL (even "stationary")
                    ==================              ======================

Distribution        External task change            Internal policy change
shift source:       (friction, class labels)        (Ï€ improves over time)

Manifold            M_Ï„â‚ â†’ M_Ï„â‚‚ â†’ M_Ï„â‚ƒ             M_Ï€â‚ â†’ M_Ï€â‚‚ â†’ ... â†’ M_Ï€*
evolution:          (discrete jumps)                (continuous drift)

Dead neurons        Hyperplane misses new           Hyperplane misses new
mechanism:          task's manifold                 policy's state manifold

Plasticity          Can't learn new task            Can't refine policy
loss symptom:                                       (performance plateau)

Margin analysis     Captures manifold-hyperplane    Same geometric insight,
purpose:            mismatch at task boundary       tracked continuously
```

**Bottom line**: Margin analysis in stationary RL tracks the same geometric phenomenon (hyperplanes drifting away from the data manifold) â€” the only difference is that the manifold shift is **continuous and internally driven** rather than **discrete and externally imposed**.

---

## 9. Implementation Status: Ant vs SlipperyAnt Experiments

### Experiment Configuration Summary

| Component | Ant (Stationary) | SlipperyAnt (Non-stationary) |
|-----------|------------------|------------------------------|
| **Environment** | `Ant-v3` | `SlipperyAnt-v3` |
| **Python Script** | `run_ppo_wandb.py` | `run_ppo.py` |
| **Submission Script** | `submit_all_ant_experiments.sh` | `submit_all_sant_experiments.sh` |
| **Worker Script** | `run_ppo_ant.sh` | `run_sant.sh` |
| **Config Directory** | `cfg/ant/` | `cfg/sant/` |
| **Available Configs** | std, ns, l2, cbp, redo | std, ns, l2, cbp |
| **Total Steps** | 100M | 20M |
| **Task Changes** | None (implicit from policy improvement) | Every 2M steps (friction change) |

### Network Architecture (Both Experiments)

| Parameter | Value |
|-----------|-------|
| Hidden layers | 2 |
| Hidden dimensions | [256, 256] |
| Activation | ReLU |
| Networks | MLPPolicy (actor) + MLPVF (critic) |

### Algorithm Configurations

| Config | Description | Key Parameters |
|--------|-------------|----------------|
| **std** | Standard PPO | Default Adam (Î²â‚=0.9, Î²â‚‚=0.999) |
| **ns** | Non-stationary Adam | Î²â‚=0.99, Î²â‚‚=0.99 (higher momentum) |
| **l2** | PPO + L2 regularization | wd=1e-3 |
| **cbp** | Continual Backprop | rr=1e-4, mt=10000 (regenerates low-utility neurons) |

### Metrics Logged

| Metric | Ant | SlipperyAnt | Description |
|--------|-----|-------------|-------------|
| `pol_features_activity` | âœ“ | âœ“ | Dormant units percentage per layer |
| `pol_weights` | âœ“ | âœ“ | Policy weight magnitude per layer |
| `val_weights` | âœ“ | âœ“ | Value network weight magnitude |
| `stable_rank` | âœ“ | âœ“ | Effective rank of representations |
| `weight_change` | âœ“ | âœ“ | Weight change tracking |
| `margin/dead_counts` | âœ“ | âœ“ | Dead neurons per layer |
| `margin/at_risk_counts` | âœ“ | âœ“ | At-risk neurons per layer |
| `margin/mean_margins` | âœ“ | âœ“ | Mean pre-activation margins |

### Margin Analysis Configuration

| Aspect | Ant (Stationary) | SlipperyAnt (Non-stationary) |
|--------|------------------|------------------------------|
| **When computed** | Every PPO update (every 2048 steps) | At task boundaries (start & end) |
| **Buffer size** | 1000 observations | 1000 observations |
| **Total computations** | ~48,828 (over 100M steps) | ~20 (2 per task Ã— 10 tasks) |
| **Network analyzed** | Policy only (`pol.mean_net`) | Policy only (`pol.mean_net`) |
| **Logged to wandb** | âœ“ | âœ“ |

### How to Run

```bash
cd /scratch/gautschi/shin283/loss-of-plasticity/lop/rl

# Ant (stationary) - 100M steps, 8 GPUs
./submit_all_ant_experiments.sh

# SlipperyAnt (non-stationary) - 20M steps, 8 GPUs
./submit_all_sant_experiments.sh
```

### Expected Output Files

```
# Ant experiments
data/ant/bp/std/{seed}.log      # Pickle with all metrics
data/ant/bp/std/{seed}.pth      # Model checkpoint
data/ant/bp/std/{seed}.done     # Completion marker

# SlipperyAnt experiments
data/sant/bp/std/{seed}.log
data/sant/bp/std/{seed}.pth
data/sant/bp/std/{seed}.done
```

### Key Research Questions

1. **Ant (stationary)**: Do neurons die even without explicit task changes? At what rate?
2. **SlipperyAnt (non-stationary)**: How do margins change at task boundaries? Does death accumulate across tasks?
3. **Algorithm comparison**: Does CBP/L2 prevent neuron death in both settings?
4. **Correlation**: Is dead neuron count correlated with performance plateau?

---

## References

1. He et al. 2025. "Spectral Collapse Drives Loss of Plasticity in Deep Continual Learning"
2. Garg et al. 2023. "Memorization Through the Lens of Curvature of Loss Function Around Samples"
3. Roy & Vetterli 2007. "The effective rank: A measure of effective dimensionality"
4. Martens & Grosse 2015. "Optimizing neural networks with Kronecker-factored approximate curvature"
